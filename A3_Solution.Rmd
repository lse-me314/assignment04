
# ME314 Managing and Visualizing Data

## Day 3 Assignment, LSE ME314 2019

---

### 1. Normalizing data

This question uses this table:
![Not normalized data](http://www.essentialsql.com/wp-content/uploads/2014/06/Intro-Table-Not-Normalized.png)

from the lecture notes.  For each answer, please explain as fully as possible, and feel free to use tables or diagrams if you prefer.

a)  Why does this table violate the first normal form, and what would be required to make it 1NF?

**The rules to satisfy 1st normal form are:**

-  **That the data is in a database table.**
-  **The table stores information in rows and columns where one or more columns, called the primary key, uniquely identify each row.**
-  **Each column contains atomic values, and there are not repeating groups of columns.**

**This table violates the 1NF because it contains data customer data in more than one column, in this case, `Customer1`, `Customer2`, and `Customer3`.**

**We could solve this by moving the Customer information to its own table, and linking its primary key to a matching foreign key in the original table related to SalesStaff.**

**See [this website](https://www.essentialsql.com/get-ready-to-learn-sql-8-database-first-normal-form-explained-in-simple-english/) for more details.**

b)  What additional steps would be needed to make the table 2NF, and why?

**A table is in 2nd Normal Form if:**
-  **The table is in 1st normal form, and**
-  **All the non-key columns are dependent on the table’s primary key.**

**We know that the tables are not in 1NF, but assuming that we had fixed that, there would still issues, for example, the `SalesStaff` table has two columns that do not depend on the EmployeeID:  `SalesOffice` and `OfficeNumber`.  We would better creating a separate table for Sales offices, and linking its primary key to the `SalesStaff` table via a matching foreign key in the `SalesStaff` table.**

**There are other potential issues that wherein, depending on how you achieved the 1NF step, there could still be table attributes that do not completely rely on that tables's primary key.  For more examples, see [here](https://www.essentialsql.com/get-ready-to-learn-sql-10-database-second-normal-form-explained-in-simple-english/).**


c)  Why might we not want to normalize data to the fullest extent possible?

**Sometimes a database table that is not fully normalized still provides sufficient information for our needs, without the complexity of full normalization providing gains that outweigh the loss of simplicity.  Third and higher form normalization are often ignored, for instance, for small-scale databases built for specific purposes.  For large scale database scheme that need the ability to be extended easily, however, and for which data integrity is critical, full normalization is generally the best strategy.**

d)  In the table below, which of the three normalization rules does this violate, if any, and why?

   |  countryID  |  countryName    |   EUmember   |  EUjoindate  |
   | -----------:|:----------------|:------------:|:------------:|
   | 00001       | France          |  `true`      |  1958-01-01  |
   | 00004       | Hungary         |  `true`      |  2004-05-01  |
   | 00003       | Serbia          |  `false`     |       `NULL` |
   | 00004       | Finland         |  `true`      |  1995-01-01  |
   | 00005       | Russia          |  `false`     |       `NULL` |
   | 00006       | Ireland, UK     |  `true`      |  1973-01-01  |

   Are there any other problems with the table, besides normalization?

**Yes:**  
-  **1NF is violated because because `countryName` contains multiple country values in the last row.**
-  **2NF is violated because we could have created a table of EU membership statuses (including soon, sadly, un-joining) that would link to the Country table via `CountryID`.**
-  **3NF is violates because the `EUmember` can be determined by whether the `EUjoindate` is not `NULL`.**

e)  What would it take to full (1NF-3NF) normalize this dataset?

   Write out these tables, and describe why this meets each of the normal forms.  This is a database of movies watched on NetBricks, a streaming movie service.

   | Name           | Address    |   Movies Rented   |  Salutation  | Category |
   |:---------------|:-----------|:------------------|:------------:|----------|
   | Bob Smith      | 1 Houghton Street    | _Star Wars_, _Inception_ |  Dr.   |  Scifi, Scifi |
   | Pry Ministair  | 10 Downing St     |  _Brexit the Movie_      |  Lady  | Tragedy |
   | Joe Bloggs     | 8 Myhatt St.      |  _Fast and Furious 6_, _Fast and Furious 7_     | Mr. | Action, Action |

**Create tables for:**

-  **User, with a `UserID`, which would record the address, and salutation**
-  **Movies, with a `MovieID`**
-  **Categories, with a `CategoryID` linking to the Movies table**
-  **Rental Table, linking to Movies and User tables, with added fields for the date rented**

```{r}
# create user table
user_table <- read.csv(textConnection("userid, Name, Address, Salutation
     1,     Bob Smith, 1 Houghton Street,        Dr.
     2, Pry Ministair,     10 Downing St,       Lady
     3,    Joe Bloggs,      8 Myhatt St.,        Mr."),
     stringsAsFactors = FALSE)

# comma-separated values = CSV
# we use read.csv() alongside textConnection because of the format being the same as when you read in a CSV file. we could use data.frame() as we did below, but would have to reorder the data as in its current state, it's the same as copy-pasting rows from excel, so it is easier to do it this way. 


# create movies table
movie_table <- data.frame(movieid = 1:5,
                          title = c("Star Wars", "Inception", "Brexit the Movie", 
                                    "Fast and Furious 6", "Fast and Furious 7"),
                          categoryid = c(1,1, 2, 3, 3),
                          stringsAsFactors = FALSE)

# create rental table
rental_table <- data.frame(userid = c(1, 1, 2, 3, 3),
                           movieid = 1:5)

# create category table
category_table <- data.frame(categoryid = 1:3,
                             genres = c("Scifi", "Tragedy", "Action"),
                             stringsAsFactors = FALSE)

# why stringsAsFactors= FALSE? let's try it with TRUE..

# anytime you have character values in a string you want to make sure you specify FALSE otherwise it will default to TRUE. Now if you have some previous experience with R, note that " The ‘factory-fresh’ default has been TRUE previously but has been changed to FALSE for R 4.0.0."
```

**Here are the four tables in fully normalized database** 

#### User table

```{r echo = FALSE, results = 'asis'}
library("knitr")
library(dplyr)
kable(user_table, caption = 'User Table')
```

#### Rental table
```{r echo = FALSE, results = 'asis'}
kable(rental_table, caption = 'Rental Table')
```

#### Movie table
```{r echo = FALSE, results = 'asis'}
kable(movie_table, caption = 'Movie Table')
```

#### Category table
```{r echo = FALSE, results = 'asis'}
kable(category_table, caption = 'Category Table')
```

**These tables fits to the 3 levels of normalization because**

- **For achieving 1NF, the problem was the multiple rental records in each row. By separating this as the rental table and making each row correspond with a single rental record, this issue is resolved**.
- **For achieving 2NF, the partial dependency of records on the primary keys would be an issue. This could be an issue if we have both movieid and movie information in rental record because the movie information can be uniquely identified by the movie id. By separating this information into a separate table (i.e. Movie Table), this issue was resolved.**
- **For achieving 3NF, the tables should not have transitive dependency. Since the movie categories does not have any information other than the titles of categories, this cannot be an issue here unless we have a secondary key of categoryid in addition to the category name, but I created a new table categoryid just in case.**

### 2.  Reshaping data

For this exercise, we will use the **nycflights13** R package, whose tables have been output in `.csv` form [here](nycflights13/).  You may do the following in either R or Python.  Note that this example is developed extensively in [_R for Data Science_](http://r4ds.had.co.nz/relational-data.html).

a)  Create a subtable of the `flights` data, that departed before 05:53 on 2013-02-28.  How many rows and columns does this subtable have?  

```{r echo = FALSE}
flights <- read.csv("nycflights13/flights.csv")
library("dplyr")
library("lubridate")
flights$dep_datetime <- sprintf("%s-%02d-%02d %02d:%02d",
                                flights$year,
                                flights$month,
                                flights$day, 
                                flights$hour,
                                flights$minute) %>% 
    lubridate::ymd_hm()

flights_sub <- flights %>% 
    filter(dep_datetime < lubridate::ymd_hm("2013-02-28 05:53"))
nrow(flights_sub)
ncol(flights_sub)
```

```{r}
flights <- read.csv("nycflights13/flights.csv")
flights_sub_day <- flights %>% 
    filter(year == 2013 & month == 2 & day == 28 & dep_time < 553)

sprintf("the selected data.frame has %s rows and %s columns\n", 
        nrow(flights_sub), ncol(flights_sub)) %>% 
    cat()

nrow(flights_sub_day)
```

b)  Merge or join the subtable from a. `flights` data, to produce a result that includes:  
   *  Departure time
   *  Carrier (two digit code, from `carrier`)
   *  Flight number
   *  Destination airport name (hint: you will need to get this from the `airports` table)  


```{r}
airports <- read.csv("nycflights13/airports.csv")
tmp <- merge(flights_sub, airports, by.x = "dest", by.y = "faa", all.x = TRUE)
answer2 <- select(tmp, c("dep_time", "carrier", "flight", "name"))
```

c)  For every airline that had flights in the `flights` data compute the average age of the planes it flew from the entire dataset.  Age here will be defined as 2013 minus the `year` variable from the `planes` data.  Hint: This involves a join operation on `tailnum`, but also a grouped mean to compute the age (and subtracting 2013, which you can do before or after the computation of the mean).


```{r}
## your code
planes <- read.csv("nycflights13/planes.csv")

tmp <- flights %>% 
  filter(!duplicated(tailnum)) %>% 
  select(c("tailnum", "carrier") ) %>% 
  left_join(planes, by = "tailnum")

airlines <- read.csv("nycflights13/airlines.csv")

tmp %>% group_by(carrier) %>% 
  summarize(mean_age = mean(2013 - year, na.rm = TRUE)) %>%
  left_join(airlines, by = "carrier") %>% 
  select(c("name", "mean_age"))
```

### 3.  Groupwork
Get into pairs or groups and think of a research question you might have using the flights data and produce some output to discuss (no more than 2-3 minutes) at the start of the next class. (You can send the code via slack to be run by Sarah or Yuanmo or connect your own laptop.)

Consult the dplyr reference of the many functions that there are: 
[dplyr reference](https://dplyr.tidyverse.org/reference/index.html)

We might be interested in looking at whether the data shows a link between weather and delayed flights, so we can do a simple lot. But first we need to merge/join the merged flights/airlines data with the weather data. 

However, weather only has data on three airports: 
EWR: Newark, NJ 
JFK: JFK airport in NYC  
LGA: LaGuardia Airport (also NYC)

We can merge weather data with 'origin' with the other merged df 'tmp'
```{r}
head(flights)

# we've done this earlier
tmp <- merge(flights, airports, by.x = "dest", by.y = "faa", all.x = TRUE)

# this time we want to keep variables indicating delays and 'time_hour', which we will need along with 'origin' to match with our weather df.
airport_flights <- select(tmp, c("dep_time", "carrier", "flight", "name", "dest", "origin", "dep_delay", "arr_delay", "time_hour"))

# This lubridate function allows us to recognize the date_time column as a date_time and not character as it was before:
airport_flights$time_hour <-  as_datetime(airport_flights$time_hour)

# Read in the weather data
weather <- read.csv("nycflights13/weather.csv")
weather$time_hour <- as_datetime(weather$time_hour)

merge_dest <- weather %>% 
  left_join(airport_flights, by.x = "dest", by.y = "origin")

merge_origin <- weather %>% 
  full_join(airport_flights)

merge_origin <- merge_origin %>% 
  select(c("origin", "temp", "humid", "wind_speed", "wind_gust", "precip", "visib", "time_hour", "dep_delay", "arr_delay")) %>% 
  na.omit()

# left join 342288 -- 326089
# inner join 335563 --- 326089
# full join 343501 --- 326089
# right join 336776 --- 326089
```
Now we want to look and see if weather impacts delays more if the inclement weather is at the destination or point of departure. 

First we can look at departure delay time (minutes) and wind speed
```{r}
# here we want to use ggplot2
library(ggplot2)
```


```{r}
#plots of the points
ggplot(merge_origin, aes(wind_speed, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm) # regression line

# The plots us what we can see with summary()
summary(merge_origin$wind_speed)


```

Let's drop the outliers so we can see the other data better:
```{r}
merge_origin_subwindspd <- subset(merge_origin, wind_speed < 1000)

# we can see that the outlier is removed in summary() output
summary(merge_origin_subwindspd$wind_speed)

# plots of the points
ggplot(merge_origin_subwindspd, aes(wind_speed, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm) # regression line

# see correlation
cor.test(merge_origin_subwindspd$wind_speed, merge_origin_subwindspd$dep_delay)
```
 
 We can also look at delays and precipitation:
 
```{r}
summary(merge_origin$precip)

#plots of the points
ggplot(merge_origin, aes(precip, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm)

precip_sub <- subset(merge_origin, precip < 0.6)

ggplot(precip_sub, aes(precip, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm)
```
The only issue with look at this on a plot, is we don't know how significant the correlation is. This is where today's lecture on regression comes in, and is what you will start doing in Assignment 4. Let's run a multiple linear regression on the data now:

```{r}
lm_weather <- lm(dep_delay ~ temp + humid + wind_speed + wind_gust + precip + visib, merge_origin)

summary(lm_weather)
```

Hey -- wind_gust is NA: multicollinearity? 

Sure enough, if we run cor() we can see that wind gust and wind speed are perfectly correlated. We want linear independence here, so we will drop one of them. 

```{r}
cor(merge_origin[sapply(merge_origin, is.numeric)])
# OR
cor(subset(merge_origin, select = c(-origin, -time_hour)))
```

```{r}
lm_weather <- lm(dep_delay ~ temp + humid + wind_speed + precip + visib, merge_origin)

summary(lm_weather)
```

The regression coefficient for precip, for instance suggests that for every unit increase in precipitation, delays increases by the coefficient (47 minutes). In other words, rain delays departure by 46 minutes per .01 increase of rain. Interestingly, humidity and visibility have a negative correlation with delays.

Looking at the R-squared, we can see that this model explains less than 2% of the variability. What this means in practical terms, is there are probably a lot of other factors influencing departure delays that we just don't have here, so this is certainly a moment to underline that "correlation does not imply causation." 

