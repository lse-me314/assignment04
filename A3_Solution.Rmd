
# ME314 Managing and Visualizing Data

## Day 3 Assignment, LSE ME314 2019

---

### 1. Normalizing data

This question uses this table:

   | EmployeeID | SalesPerson | SalesOffice | OfficeNumber | Customer1 | Customer2 | Customer3 |
   |-------:|:-------------:|:----------:|:------------ |:-------:|:------:|:------:|
   | 1003   | Mary Smith    |  Chicago   | 312-555-1212 | Ford   | GM |       |
   | 1004   | John Hunt     |  New York  | 212-555-1212 | Dell   | HP | Apple |
   | 1005   | Martin Hap    |  Chicago   | 312-555-1212 | Boeing |    |       |

from the lecture notes.  For each answer, please explain as fully as possible, and feel free to use tables or diagrams if you prefer.

a)  Why does this table violate the first normal form, and what would be required to make it 1NF?

**The rules to satisfy 1st normal form are:**

-  **That the data is in a database table.**
-  **The table stores information in rows and columns where one or more columns, called the primary key, uniquely identify each row.**
-  **Each column contains atomic values, and there are not repeating groups of columns.**

**This table violates the 1NF because it contains data customer data in more than one column, in this case, `Customer1`, `Customer2`, and `Customer3`.**

**We could solve this by moving the Customer information to its own table, and linking its primary key to a matching foreign key in the original table related to SalesStaff.**

**Employee table:**

- **Primary key = EmployeeID**

   | EmployeeID | SalesPerson | SalesOffice | OfficeNumber |
   |-------:|:-------------:|:----------:|:-------------|
   | 1003   | Mary Smith    |  Chicago   | 312-555-1212 |
   | 1004   | John Hunt     |  New York  | 212-555-1212 |
   | 1005   | Martin Hap    |  Chicago   | 312-555-1212 |

**Customer table:**

- **Primary key = Customer**
- **Foreign key = EmployeeID**

   | Customer | EmployeeID | 
   |:--------:|------:|
   | Ford     | 1003  | 
   | GM       | 1003  | 
   | Dell     | 1004  | 
   | HP       | 1004  | 
   | Apple    | 1004  | 
   | Boeing   | 1005  | 

b)  What additional steps would be needed to make the table 2NF? What about 3NF?

**A table is in 2nd Normal Form if:**

- **The table is in 1st normal form, and**
- **All the non-key columns are 'dependent' on the tableâ€™s primary key (and not a subset of the table's primary key).**

**To achieve this, remove data that is only dependent on part of the primary key. If the candidate key comprises of only single attribute and relation is in 1NF then it is already in 2NF.**

**Let's work with the solution to (a). Since we do not have a compound key, it is automatically in 2NF.**

**However, it is not in 3NF. A table is in 3rd Normal Form if:**

1. **It is in second normal form (2NF).**
2. **No non-prime attribute is transitively dependent on prime key attribute.**

**We have established that condition 1 holds. What about condition 2? An attribute Y is dependent on an attribute X if knowing X is sufficient to know Y. So for example, if we know the EmployeeID in the Employee table, we will be able to identify the SalesPerson. For more information on this concept of 'dependency', see this tutorial:** https://www.tutorialspoint.com/dbms/database_normalization.htm

**A transitive dependency is one that goes through another attribute. For example, if we know the EmployeeID, we will be able to identify the OfficeNumber; and if we know the OfficeNumber, we will be able to identify the SalesOffice (so, EmployeeID -> OfficeNumber -> SalesOffice). Where this is present (and the table is also in 2NF) there is likely to be redundant data duplication.**

**Since a transitive dependency is present in the employee table, the employee table violates 3NF. We could fix this by removing the SalesOffice column from the employee table and creating a separate Office table, where OfficeNumber is the primary key (and SalesOffice is a non-key attribute).**

**3NF Employee table:**

- **Primary key = EmployeeID**
- **Foreign key = OfficeNumber**

   | EmployeeID | SalesPerson | OfficeNumber |
   |-------:|:----------------|:-------------|
   | 1003   | Mary Smith      | 312-555-1212 |
   | 1004   | John Hunt       | 212-555-1212 |
   | 1005   | Martin Hap      | 312-555-1212 |

**3NF Office table:**

- **Primary key = OfficeNumber**

   | OfficeNumber  | SalesOffice |
   |--------------:|:------------|
   | 212-555-1212  | New York    |
   | 312-555-1212  | Chicago     |

**Having normalized the data in this way, we have now minimised the amount of data duplication, therefore reducing the memory required to store the data and making it easier to update information. For example, if for some reason the Chicago office (i.e. the office with OfficeNumber: 312-555-1212) relocated to Houston, we now just update one cell in the Office table - whereas in the original table we would have needed to update two cells.**

c)  Why might we not want to normalize data to the fullest extent possible?

**Sometimes a database table that is not fully normalized still provides sufficient information for our needs, without the complexity of full normalization providing gains that outweigh the loss of simplicity.  Third and higher form normalization are often ignored, for instance, for small-scale databases built for specific purposes.  For large scale database scheme that need the ability to be extended easily, however, and for which data integrity is critical, full normalization is generally the best strategy.**

d)  In the table below, which of the three normalization rules does this violate, if any, and why?

   |  countryID  |  countryName    |   EUmember   |  EUjoindate  |
   | -----------:|:----------------|:------------:|:------------:|
   | 00001       | France          |  `true`      |  1958-01-01  |
   | 00004       | Hungary         |  `true`      |  2004-05-01  |
   | 00003       | Serbia          |  `false`     |       `NULL` |
   | 00004       | Finland         |  `true`      |  1995-01-01  |
   | 00005       | Russia          |  `false`     |       `NULL` |
   | 00006       | Ireland, UK     |  `true`      |  1973-01-01  |

   Are there any other problems with the table, besides normalization?

**Yes:**  
-  **1NF is violated because because `countryName` contains multiple country values in the last row.**
-  **3NF is violates because the `EUmember` can be determined by whether the `EUjoindate` is not `NULL`. This means there is a transitive dependency (countryID -> EUjoindate -> EUmember).**

e)  What would it take to full (1NF-3NF) normalize this dataset?

   Write out these tables, and describe why this meets each of the normal forms.  This is a database of movies watched on NetBricks, a streaming movie service.

   | Name           | Address    |   Movies Rented   |  Salutation  | Category | Fiction? |
   |:---------------|:-----------|:------------------|:------------:|----------|----------|
   | Bob Smith      | 1 Houghton Street  | _Star Wars_, _Fast and Furious 7_ |  Dr. |  Scifi, Action | Yes, Yes|
   | Pry Ministair  | 10 Downing St  |  _Brexit: Three Years On_   |  Rt Hon.  | Documentary | No |
   | Joe Bloggs     | 8 Myhatt St.  |  _The Big Short_, _Star Wars_  | Mr. | Documentary, Scifi | No, Yes |

**Create tables for:**

-  **User, with a `UserID`, which would record the address, and salutation**
-  **Movies, with a `MovieID` and the movie's name**
-  **Categories, with information about which movie categories are fiction or non-fiction**
-  **Rental Table, linking to Movies and User tables**

```{r}

library(tidyverse)

# 1NF...
# First, separate out values in 'movies rented' and 'category' so that they are atomic:
# Second, make longer so that each column represents a single attribute

rental_table_1NF <- tribble(
  ~userID, ~movieID, ~Name, ~Address, ~`Movies Rented`, ~Salutation, ~Category, ~`Fiction?`,
  #------|---------|------|---------|-----------------|------------|----------|-----------
  "001",  "1", "Bob Smith", "1 Houghton Street", "Star Wars",  "Dr.", "Scifi", "Yes",
  "001",  "2", "Bob Smith", "1 Houghton Street", "Fast and Furious 7",  "Dr.", "Action", "Yes",
  "002",  "3", "Pry Ministair", "10 Downing St", "Brexit: Three Years On",  "Rt Hon.", "Documentary", "No",
  "003",  "4", "Joe Bloggs", "8 Myhatt St.", "The Big Short",  "Mr.", "Documentary", "No",
  "003",  "1", "Joe Bloggs", "8 Myhatt St.", "Star Wars",  "Mr.", "Scifi", "Yes"
)

# 2NF...
# Primary key is the conjunction of userID and movieID
# Name, Address and Salutation are dependent on userID
# Movies Rented and Category are dependent on movieID
# We can fix this by creating separate user and movie tables and removing these columns from the main table

rental_table_2NF <- rental_table_1NF %>%
  select(c(userID, movieID))
  
user_table_2NF <- rental_table_1NF %>%
  select(c(userID, Name, Address, Salutation)) %>%
  filter(!duplicated(.))

movie_table_2NF <- rental_table_1NF %>%
  select(c(movieID, `Movies Rented`, Category, `Fiction?`)) %>%
  filter(!duplicated(.))

# 3NF...
# There is a transitive dependency in that `Fiction?` can be determined by `Category`
# We can fix this by separating information about category from information about movies rented

movie_table_3NF <- rental_table_1NF %>%
  select(c(movieID, `Movies Rented`, Category))

category_table_3NF <- rental_table_1NF %>%
  select(c(Category, `Fiction?`)) %>%
  filter(!duplicated(.))

# The other tables meet 2NF already

rental_table_3NF <- rental_table_2NF

user_table_3NF <- user_table_2NF

```

**Here are the four tables in fully normalized database** 

#### User table

```{r echo = FALSE, results = 'asis'}
library("knitr")
library(dplyr)
kable(user_table_3NF, caption = 'User Table')
```

#### Rental table
```{r echo = FALSE, results = 'asis'}
kable(rental_table_3NF, caption = 'Rental Table')
```

#### Movie table
```{r echo = FALSE, results = 'asis'}
kable(movie_table_3NF, caption = 'Movie Table')
```

#### Category table
```{r echo = FALSE, results = 'asis'}
kable(category_table_3NF, caption = 'Category Table')
```

**These tables fits to the 3 levels of normalization because**

- **To achieve 1NF, we need to ensure that every attribute has its own column and all values are atomic. This is met in all the tables displayed.**

- **To achieve 2NF, we need to ensure that after conforming it to 1NF, every non-prime attribute depends on the primary key (and not a subset of the primary key). In the workings above, we first converted the table into 1NF by making a table with a compound primary key composed of userID and movieID. This turned out not to conform to 2NF because some attributes depended on one (rather than both) of the primary attributes. By separating this information into separate tables (i.e. Movie Table and User Table), this issue was resolved.**

- **To achieve 3NF, the tables should not have transitive dependency. Since whether a movie is fiction can be determined by its genre, this is present in the original table (and the intermediate 2NF Movie Table). To resolve this, we moved the information about movie categories into a separate table.**

### 2.  Reshaping data

For this exercise, we will use the **nycflights13** R package, whose tables have been output in `.csv` form [here](nycflights13/).  You may do the following in either R or Python.  Note that this example is developed extensively in [_R for Data Science_](http://r4ds.had.co.nz/relational-data.html).

a)  Create a subtable of the `flights` data, that departed before 05:53 on 2013-02-28.  How many rows and columns does this subtable have?  

```{r echo = FALSE}

flights <- read.csv("nycflights13/flights.csv")
library("dplyr")
library("lubridate")
flights$dep_datetime <- sprintf("%s-%02d-%02d %02d:%02d",
                                flights$year,
                                flights$month,
                                flights$day, 
                                flights$hour,
                                flights$minute) %>% 
    lubridate::ymd_hm()

flights_sub <- flights %>% 
    filter(dep_datetime < lubridate::ymd_hm("2013-02-28 05:53"))
nrow(flights_sub)
ncol(flights_sub)
```

```{r}
flights <- read.csv("nycflights13/flights.csv")
flights_sub_day <- flights %>% 
    filter(year == 2013 & month == 2 & day == 28 & dep_time < 553)

sprintf("the selected data.frame has %s rows and %s columns\n", 
        nrow(flights_sub), ncol(flights_sub)) %>% 
    cat()

nrow(flights_sub_day)
```

b)  Merge or join the subtable from a. `flights` data, to produce a result that includes:  
   *  Departure time
   *  Carrier (two digit code, from `flights`)
   *  Flight number
   *  Destination airport name (hint: you will need to get this from the `airports` table)  


```{r}
airports <- read.csv("nycflights13/airports.csv")
tmp <- merge(flights_sub, airports, by.x = "dest", by.y = "faa", all.x = TRUE)
answer2 <- select(tmp, c("dep_time", "carrier", "flight", "name"))
```

c)  For every airline that had flights in the `flights` data compute the average age of the planes it flew from the entire dataset.  Age here will be defined as 2013 minus the `year` variable from the `planes` data.  Hint: This involves a join operation on `tailnum`, but also a grouped mean to compute the age (and subtracting 2013, which you can do before or after the computation of the mean).


```{r}
## your code
planes <- read.csv("nycflights13/planes.csv")

tmp <- flights %>% 
  filter(!duplicated(tailnum)) %>% 
  select(c("tailnum", "carrier") ) %>% 
  left_join(planes, by = "tailnum")

airlines <- read.csv("nycflights13/airlines.csv")

tmp %>% group_by(carrier) %>% 
  summarize(mean_age = mean(2013 - year, na.rm = TRUE)) %>%
  left_join(airlines, by = "carrier") %>% 
  select(c("name", "mean_age"))
```

### 3.  Groupwork
Get into pairs or groups and think of a research question you might have using the flights data and produce some output to discuss (no more than 2-3 minutes) at the start of the next class. (You can send the code via slack to be run by Sarah or Yuanmo or connect your own laptop.)

Consult the dplyr reference of the many functions that there are: 
[dplyr reference](https://dplyr.tidyverse.org/reference/index.html)

We might be interested in looking at whether the data shows a link between weather and delayed flights, so we can do a simple lot. But first we need to merge/join the merged flights/airlines data with the weather data. 

However, weather only has data on three airports: 
EWR: Newark, NJ 
JFK: JFK airport in NYC  
LGA: LaGuardia Airport (also NYC)

We can merge weather data with 'origin' with the other merged df 'tmp'
```{r}
head(flights)

# we've done this earlier
tmp <- merge(flights, airports, by.x = "dest", by.y = "faa", all.x = TRUE)

# this time we want to keep variables indicating delays and 'time_hour', which we will need along with 'origin' to match with our weather df.
airport_flights <- select(tmp, c("dep_time", "carrier", "flight", "name", "dest", "origin", "dep_delay", "arr_delay", "time_hour"))

# This lubridate function allows us to recognize the date_time column as a date_time and not character as it was before:
airport_flights$time_hour <-  as_datetime(airport_flights$time_hour)

# Read in the weather data
weather <- read.csv("nycflights13/weather.csv")
weather$time_hour <- as_datetime(weather$time_hour)

merge_dest <- weather %>% 
  left_join(airport_flights, by.x = "dest", by.y = "origin")

merge_origin <- weather %>% 
  full_join(airport_flights)

merge_origin <- merge_origin %>% 
  select(c("origin", "temp", "humid", "wind_speed", "wind_gust", "precip", "visib", "time_hour", "dep_delay", "arr_delay")) %>% 
  na.omit()

# left join 342288 -- 326089
# inner join 335563 --- 326089
# full join 343501 --- 326089
# right join 336776 --- 326089
```
Now we want to look and see if weather impacts delays more if the inclement weather is at the destination or point of departure. 

First we can look at departure delay time (minutes) and wind speed
```{r}
# here we want to use ggplot2
library(ggplot2)
```


```{r}
#plots of the points
ggplot(merge_origin, aes(wind_speed, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm) # regression line

# The plots us what we can see with summary()
summary(merge_origin$wind_speed)


```

Let's drop the outliers so we can see the other data better:
```{r}
merge_origin_subwindspd <- subset(merge_origin, wind_speed < 1000)

# we can see that the outlier is removed in summary() output
summary(merge_origin_subwindspd$wind_speed)

# plots of the points
ggplot(merge_origin_subwindspd, aes(wind_speed, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm) # regression line

# see correlation
cor.test(merge_origin_subwindspd$wind_speed, merge_origin_subwindspd$dep_delay)
```
 
 We can also look at delays and precipitation:
 
```{r}
summary(merge_origin$precip)

#plots of the points
ggplot(merge_origin, aes(precip, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm)

precip_sub <- subset(merge_origin, precip < 0.6)

ggplot(precip_sub, aes(precip, dep_delay, colour = visib)) + 
  geom_point() + 
  geom_smooth(method = lm)
```
The only issue with look at this on a plot, is we don't know how significant the correlation is. This is where today's lecture on regression comes in, and is what you will start doing in Assignment 4. Let's run a multiple linear regression on the data now:

```{r}
lm_weather <- lm(dep_delay ~ temp + humid + wind_speed + wind_gust + precip + visib, merge_origin)

summary(lm_weather)
```

Hey -- wind_gust is NA: multicollinearity? 

Sure enough, if we run cor() we can see that wind gust and wind speed are perfectly correlated. We want linear independence here, so we will drop one of them. 

```{r}
cor(merge_origin[sapply(merge_origin, is.numeric)])
# OR
cor(subset(merge_origin, select = c(-origin, -time_hour)))
```

```{r}
lm_weather <- lm(dep_delay ~ temp + humid + wind_speed + precip + visib, merge_origin)

summary(lm_weather)
```

The regression coefficient for precip, for instance suggests that for every unit increase in precipitation, delays increases by the coefficient (47 minutes). In other words, rain delays departure by 46 minutes per .01 increase of rain. Interestingly, humidity and visibility have a negative correlation with delays.

Looking at the R-squared, we can see that this model explains less than 2% of the variability. What this means in practical terms, is there are probably a lot of other factors influencing departure delays that we just don't have here, so this is certainly a moment to underline that "correlation does not imply causation." 

